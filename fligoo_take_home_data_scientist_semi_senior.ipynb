{
 "cells": [
  {
   "attachments": {
    "new-logo-32.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![new-logo-32.png](attachment:new-logo-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style=\"font-size:40px;text-align:center\">Take Home - Data Science</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hotel Bookings Data\n",
    "Letâ€™s use hotel bookings data from [Antonio, Almeida, and Nunes (2019)](https://www.sciencedirect.com/science/article/pii/S2352340918315191?via%3Dihub) to predict which hotel stays included children and/or babies, based on the other characteristics of the stays such as which hotel the guests stay at, how much they pay, etc.\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/fligoo.data-science/TechInterviews/HotelBookings/header.png\"/>\n",
    "\n",
    "One of the hotels (H1) is a resort hotel and the other is a city hotel (H2). Both datasets share the same structure, with 23 variables describing the 19248 observations of H1 and 30752 observations of H2. Each observation represents a hotel booking. Both datasets comprehend bookings due to arrive between the 1st of July of 2015 and the 31st of August 2017, including bookings that effectively arrived and bookings that were canceled. Since this is hotel real data, all data elements pertaining hotel or customer identification were deleted.\n",
    "\n",
    "**Take-Home Goals**\n",
    "\n",
    "#### Part 1\n",
    "During **Part I**, you should perform an Exploratory Data Analysis highlighting key findings:\n",
    "  - Data Quality Check: You must check the quality of the given dataset to make an assessment of how appropriate it is for later Data Science tasks. Propose a set of corrective actions over the data if any.\n",
    "  - Report insights and conclusions: Describe the results obtained during the exploratory analysis and provide conclusions from a business perspective, supported by plots / tables / metrics.\n",
    "  - **Expected**:\n",
    "    - Make at least 10 plots with any plotting library (plotly, matplotlib, seaborn, etc.)\n",
    "    - Write down the conclusions, in a clear manner, of every plot in this notebook\n",
    "\n",
    "#### Part 2\n",
    "In **Part II** you should define and train a model to predict which actual hotel stays included children/babies, and which did not:\n",
    "  - **Feature extraction:** Indicate some possible candidates of features that could properly describe the hotels, either from the given columns or from their transformations.\n",
    "      - **Expected**:\n",
    "        - Create **one** scikit-learn pipeline inside a file called `pipelines.py`\n",
    "        - Create at least **three** scikit-learn transformers inside a file called `transformers.py` and use them inside the pipeline from previous step. This transformers should add new features or clean the original dataframe of this take-home\n",
    "          - Feature example: Compute \"total_nights\" feature. This is the sum of `stays_in_week_nights` + `stays_in_weekend_nights`\n",
    "          - Cleaning example: Transform string values. `'0'` to int type `0` \n",
    "        - Import pipeline and run the transformations inside this notebook\n",
    "\n",
    "  - **Machine Learning modeling:** Fit models with the given data. Pay attention to the entire process to avoid missing any crucial step. You could use the `children` column as target.\n",
    "    - **Expected**:\n",
    "      - Use the dataset with the new features generated to train *at least* **three** different machine learning models and generate metrics about their performance.\n",
    "    \n",
    "#### Part 3\n",
    "Finally, on **Part III** you should present the key findings, conclusions and results to non-technical stakeholders.\n",
    "  - **Expected**:\n",
    "    - Create a summary of all the findings in part 1\n",
    "    - Create an explanation of the features added in part 2\n",
    "    - Create a summary of the model metrics\n",
    "    - These explanations should be at high level and understood by a non-technical person\n",
    "    - You can add all the summaries and explanations at the end of this notebook, it can be done in markdown format or any other external resource like a ppt presentation, pdf document, etc. Whatever works best for you!\n",
    "\n",
    " \n",
    "**Requirements**\n",
    "- Python 3.x & Pandas 1.x\n",
    "- Paying attention to the details and narrative is far way more important than extensive development.\n",
    "- Once you complete the assessment, please send a ZIP file of the folder with all the resources used in this work (e.g. Jupyter notebook, Python scripts, text files, images, etc) or share the repository link.\n",
    "- Virtualenv, requirements or Conda environment for isolation.\n",
    "- Have a final meeting with the team to discuss the work done in this notebook and answer the questions that could arise.\n",
    "- Finally, but most important: Have fun!\n",
    "\n",
    "**Nice to have aspects**\n",
    "- Code versioning with Git (you are free to publish it on your own Github/Bitbucket account!).\n",
    "- Show proficiency in Python: By showing good practices in the structure and documentation, usage of several programming paradigms (e.g. imperative, OOP, functional), etc.\n",
    "- Shap Model explainability: explain feature importance with the use of shapley values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from pipelines import <your_pipeline_name>\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "hotels = pd.read_csv('https://s3-us-west-2.amazonaws.com/fligoo.data-science/TechInterviews/HotelBookings/hotels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels.hotel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the arrival_date dtype from object to datetime:\n",
    "hotels[\"arrival_date\"] = pd.to_datetime(hotels[\"arrival_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels.children.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=hotels, x='children', hue='children', palette='Set2')\n",
    "plt.title('Number of Bookings with Children')\n",
    "plt.xlabel('Children Value')\n",
    "plt.ylabel('Number of Bookings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire dataset is dominated by observations where there are no children, we call this behavior of the target variable \"class imbalance\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### about the target variable:\n",
    "we see the children column(which is the target variable) is a categorical variable, we are not interested on knowing how many children will there be, we are just interested on knowing if there'll be any children or not, this defines the problem as classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels[hotels['country'].isna()].hotel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels[hotels['hotel'] == \"Resort_Hotel\"].country.value_counts(dropna=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = hotels[hotels['hotel'] == \"Resort_Hotel\"].country.value_counts(dropna=False).head(25)\n",
    "country_counts.index = country_counts.index.fillna('Missing')\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "colors = ['skyblue' if country != 'Missing' else 'salmon' for country in country_counts.index]\n",
    "\n",
    "bars = plt.bar(country_counts.index, country_counts.values, color=colors)\n",
    "\n",
    "plt.xlabel('Country', fontsize=14)\n",
    "plt.ylabel('Number of Bookings', fontsize=14)\n",
    "plt.title('Number of Bookings by Country for Resort Hotel', fontsize=16)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# add counts on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 5, \n",
    "             f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Bookings by Country for Resort Hotel plot\n",
    "the main idea of the plot from above is to understand in which part of the distribution of the resort hotel do the missing values fall, actually fall, we actually end up seeing with this how it compares to the rest of the, it ends up being one of the top countries for the resort hotel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels[hotels.hotel == \"Resort_Hotel\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels[(hotels.hotel == \"Resort_Hotel\") & (hotels.country.isna())].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### about the few null values in the country column:\n",
    "there is just one column with nulls, this columns is country and from 50000 observations we can find only 289 nulls, which is almost 0.6%, after seeing the means of rows with nulls and rows without nulls, I would guess that the distribution of rows with nulls come from a different distribution and the missingness is not at random, this would normally point to the NAs being an actual source of extra information, it begs the question: why are these values null? there can be many reasons for that, but I think the best step to take those rows seriously would be to analyze the way the data was collected initially, maybe if the origin countries of the bookers dissolved (like yugoslavia) then it made more sense to keep it blank as the country code was deprecated, it could also be interesting to apply a model to predict these countries and analyze the results. because it actually is a very small percentage of the dataset I think it would make sense to test the actual missingness of a values as another value of the country column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels['country_missing'] = hotels['country'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### about duplicates:\n",
    "I don't think this is a dataset in which we have to worry about duplicates, because each observation is a stay, and because we don't a booker_id variable, we can assume that each duplicate is a booking from a different person, of the same duration, with the same arrival date, of the same country, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_columns(data: pd.DataFrame):\n",
    "    color_palette = {\n",
    "        'children': '#1f77b4',\n",
    "        'none': '#ff7f0e'\n",
    "    }\n",
    "\n",
    "    target = 'children'\n",
    "    excluded_columns = [target, 'arrival_date', 'country_missing']\n",
    "\n",
    "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = data.select_dtypes(include=['object', 'bool', 'datetime64[ns]']).columns.tolist()\n",
    "\n",
    "    numerical_cols = [col for col in numerical_cols if col not in excluded_columns]\n",
    "    categorical_cols = [col for col in categorical_cols if col not in excluded_columns]\n",
    "\n",
    "    print(f\"Numerical Columns: {numerical_cols}\")\n",
    "    print(f\"Categorical Columns: {categorical_cols}\")\n",
    "\n",
    "    cols = 3\n",
    "    total_vars = len(numerical_cols) + len(categorical_cols)\n",
    "    rows = math.ceil(total_vars / cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    def plot_numerical(ax, col):\n",
    "        sns.histplot(\n",
    "            data=data, \n",
    "            x=col, \n",
    "            hue=target, \n",
    "            element='step', \n",
    "            stat='density',\n",
    "            common_norm=False, \n",
    "            alpha=0.5, \n",
    "            ax=ax,\n",
    "            palette=color_palette, # apply color palette\n",
    "            hue_order=['children', 'none']\n",
    "        )\n",
    "        ax.set_title(f'Distribution of {col} by Children')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Density')\n",
    "\n",
    "    def plot_categorical(ax, col):\n",
    "        counts = data.groupby([col, target]).size().reset_index(name='counts')\n",
    "        \n",
    "        max_categories = 10\n",
    "        if counts[col].nunique() > max_categories:\n",
    "            top_categories = counts[col].value_counts().nlargest(max_categories).index\n",
    "            counts = counts[counts[col].isin(top_categories)]\n",
    "            counts[col] = counts[col].astype(str)\n",
    "        \n",
    "        counts['relative_freq'] = counts['counts'] / counts.groupby(target)['counts'].transform('sum')\n",
    "        \n",
    "        sns.barplot(\n",
    "            data=counts, \n",
    "            x=col, \n",
    "            y='relative_freq', \n",
    "            hue=target, \n",
    "            alpha=0.7, \n",
    "            ax=ax,\n",
    "            palette=color_palette,  # apply color palette\n",
    "            hue_order=['children', 'none']\n",
    "        )\n",
    "        ax.set_title(f'Distribution of {col} by Children')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Relative Frequency')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.legend(title=target)\n",
    "\n",
    "    # plot numerical columns\n",
    "    index = 0\n",
    "    for col in numerical_cols:\n",
    "        if index >= len(axes):\n",
    "            break\n",
    "        plot_numerical(axes[index], col)\n",
    "        index += 1\n",
    "\n",
    "    # plot categorical columns\n",
    "    for col in categorical_cols:\n",
    "        if index >= len(axes):\n",
    "            break\n",
    "        plot_categorical(axes[index], col)\n",
    "        index += 1\n",
    "\n",
    "    # delete unused axes (if there is any)\n",
    "    for i in range(index, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_columns(hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of relative frequency\n",
    "The above plots of relative frequency can be used to compare how the distributions behave side by side. For example, when examining the plots titled \"Distribution of average_daily_rate by Children,\" \"Distribution of adults by Children,\" \"Distribution of country by Children,\" and \"Distribution of reserved_room_type by Children,\" we can see very different behaviors between the distributions where Children = \"none\" and Children = \"children.\" While the absolute distributions can be very different due to class imbalance, these relative frequency graphs can tell us a lot about the proportion differences.\n",
    "\n",
    "There are also extreme outliers in the dataset. To deal at least somewhat with the most extreme cases, let's remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = stats.zscore(hotels.select_dtypes(include=[float, int]))\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 5).all(axis=1) # Cut the outliers that are 5 standard deviations away from the mean.\n",
    "clean_hotels = hotels[filtered_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_columns(clean_hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the final distribution plots, updated with the removed outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the arrival_date dtype from object to datetime\n",
    "hotels[\"arrival_date\"] = pd.to_datetime(hotels[\"arrival_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality check\n",
    "The quality of the data is very close to ideal, the missing values are pretty much non-existant which makes a very clean dataset, and even the few nulls(0.6%) found can have some predictive power. The presence of outliers can be easily detected when looking at the distribution plots and looks like the place where there is some work to do. Also, duplicates definitely make sense in this dataset.\n",
    "\n",
    "#### Corrective actions\n",
    "+ Utilized Z-score analysis to identify extreme outliers in numerical features.\n",
    "+ Converted arrival_date from dtype object to datetime\n",
    "\n",
    "## Insights\n",
    "+ We can find class imbalance in target variable (~1:11).\n",
    "+ The nulls in the country column look very interesting, there are not that many and it looks like they could potentially have some predictive power as they are definitely not missing at random. \n",
    "+ If we define two datasets, one with children = \"children\" and another with children = \"none\", we find very different behaviors in some of the relative distributions. From this piece of information one can expect good predictive power from a model.\n",
    "\n",
    "## Conclusion\n",
    "+ Dataset quality is High.\n",
    "+ High expectations of predictive power.\n",
    "+ Only a low number of missing values, may require to research the data source.\n",
    "\n",
    "With the understanding that this is an important problem to solve and that we find ways to work through it, we'll be proceeding to work through the modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Modeling\n",
    "\n",
    "We will build the best model to predict which actual hotel stays included children/babies, and which did not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Develop Machine/Statistical Learning models to predict the target variable...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pipelines import create_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hotels.drop('children', axis=1)\n",
    "y = hotels['children'].map({'children': 1, 'none': 0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "pipeline = create_pipeline()\n",
    "pipeline.fit(X_train)\n",
    "\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'XGBoost': XGBClassifier(scale_pos_weight=scale_pos_weight, eval_metric='logloss', random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "model_performance = {}\n",
    "target_names = [\"No Children\", \"Children Included\"]\n",
    "model_metrics = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test_transformed)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.decision_function(X_test_transformed)\n",
    "        y_proba = (y_proba - y_proba.min()) / (y_proba.max() - y_proba.min())\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1_score = report['1']['f1-score']\n",
    "    \n",
    "    model_metrics.append({\n",
    "        'Model': name,\n",
    "        'Precision': round(precision, 2),\n",
    "        'Recall': round(recall, 2),\n",
    "        'F1-Score': round(f1_score, 2),\n",
    "        'AUC': round(auc, 2)\n",
    "    })\n",
    "    \n",
    "    print(f\"{name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    print(\"\\n\")\n",
    "\n",
    "metrics_df = pd.DataFrame(model_metrics)\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Metrics Summary:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III - Results & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# List your key insights / findings and conclusions...\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of EDA\n",
    "High Data Quality: The dataset is predominantly complete, with only a minimal percentage (0.6%) of missing values in the country column. This low level of missing data ensures reliability in our analysis and modeling efforts.\n",
    "\n",
    "No need to manage duplicates: Each record represents a unique hotel stay, eliminating concerns about duplicate entries that could skew results.\n",
    "\n",
    "Outliers Managed: Extreme values in numerical features were identified and appropriately handled to prevent distortions in the analysis and model performance.\n",
    "\n",
    "Class Imbalance: The majority of hotel bookings do not include children (children = none). Specifically, the ratio is approximately 1:11, indicating that only about 9% of bookings involve children. This imbalance is crucial as it affects how models learn and predict outcomes.\n",
    "\n",
    "## Explanation of features added\n",
    "Number of Adults (adult_is_0 and adult_is_1): two variables have been created from the number of adults. one that tells us if there are no adults in the booking, and another that tells us if there is one adult in the booking.\n",
    "\n",
    "Number of special requests (total_of_special_requests): Two variables have been created from the number of special requests, one of these variables recognizes if the number of special requests is 0 and the other recognizes if the number of special requests is 2 or more.\n",
    "\n",
    "Distribution Channel (distribution_channel): Two variables have been created, one recognizing if the booking's distribution channel is corporate and the other recognizing if its distribution channel is direct.\n",
    "\n",
    "Data Type Conversions: Ensured that all features are in appropriate numeric formats, facilitating seamless integration with machine learning models.\n",
    "\n",
    "## Model Metrics Summary\n",
    "Four distinct models were developed to predict the presence of children in hotel bookings, here you can find a summary of the models' metrics:\n",
    "\n",
    "| Model              | Precision | Recall   | F1-Score | AUC  |\n",
    "| ------------------ | --------- | -------- | -------- | ---- |\n",
    "| LogisticRegression | 0.69      | 0.35     | 0.46     | 0.84 |\n",
    "| DecisionTree       | 0.72      | 0.34     | 0.46     | 0.79 |\n",
    "| XGBoost            | 0.28      | 0.69     | 0.40     | 0.84 |\n",
    "| RandomForest       | 0.53      | 0.44     | 0.48     | 0.79 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
